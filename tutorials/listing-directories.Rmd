---
title: "Listing contents of directories"
author: "Welliton Souza"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
# Change the value of the parameters below.
# files TSV files generated by command line (see Listing files via bash)
# names to differenciate data from TSV files (e.g. serve1, server2)
# Do not remove the brackets. Use commas as separator.
params:
  files: [~/data_home.tsv]
  names: [file1]
---

# Listing files via bash

The command line below should be executed before compiling this document.
This command line will list all existing files in a directory and compute the file size.
The output is written int a text file formated as Tab Separated Values (TSV).

```{bash, eval=FALSE}
sudo find /home/ -type f -exec du -ab {} + > data_home.tsv
```

It is possible to run this command line for different directories by changing the arguments:

- `/home/` directory to be listed. It is recomended to use __absolute path__;
- `data_home.tsv` output file name;

`sudo` is only necessary if the user does not have permission to read all files.
The output file contains only two columns, file size (in bytes) and file path.
Each line represents a file.
There is no header line.
The command line was tested in Ubuntu Linux.

# Computation

Reading TSV files into R

```{r pkgs, message=FALSE}
library(dplyr)
library(DT)
library(ggplot2)
library(htmltools)
library(plotly)

read_file <- function(file, name)
{
  data <- read.table(file, sep="\t", col.names = c("size", "file"), stringsAsFactors = FALSE)
  cbind(name, data)
}
subsetByName <- function(total.ext, name)
{
    total.ext[total.ext$name == name, ]
}
sortBySize <- function(total.ext)
{
    arrange(total.ext, -size.total.gb)
}
plotByName <- function(total.ext, name)
{
    subsetByName(total.ext, name) %>%
        filter(size.total.gb >= 1) %>%
        ggplot(aes(x=reorder(ext, -size.total.gb), y=size.total.gb)) + 
        geom_bar(stat="identity")
}
subsetFilesByName <- function(data, name)
{
    data[data$name == name, ] %>%
        mutate(size.gb = size  / 1024 / 1024 / 1014) %>%
        filter(size.gb >= 1) %>%
        arrange(-size.gb) %>%
        select(-size, -name)
}

if (length(params$files) != length(params$names))
    stop("Number of TSV files is different of the number of names.")
data <- mapply(read_file, params$files, name=params$names, SIMPLIFY = FALSE, USE.NAMES = FALSE)
data <- do.call(rbind, data)
```

Classify files by their extension
```{r}
names.file <- tolower(basename(data$file))
ext <- sub(".+\\.(\\w{,5})$", "\\1", names.file)
ext[ext == names.file] <- "OTHERS"
ext[ext == "gz" & grepl("fastq", names.file, ignore.case = TRUE)] <- "fastq.gz"
data$ext <- factor(ext)
```

Summarize entries by file extension and name.
```{r}
total.ext <- group_by(data, ext, name) %>% 
    summarise(size.total = sum(size), count = n()) %>%
    mutate(size.total.gb = size.total / 1024 / 1024 / 1014)
```

# For all TSV files

What is the top 20 file extension with more files in all TSV files?
```{r}
head(sort(table(data$ext), decreasing = TRUE), n=20)
```

# For each TSV file

```{r}
tagList(lapply(params$names, function(name) {
        list(h2(name),
        h3("Total file size (GB)"),
        datatable(sortBySize(subsetByName(total.ext, name))),
        h3("Plot of total file size by extension (>= 1GB"),
        ggplotly(plotByName(total.ext, name)),
        h3("Table containing all files with more than 1 GB"),
        datatable(subsetFilesByName(data, name)))
}))
```

